{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from lstm import LSTMNet, LSTMNetParams, prepare_data as prepare_data_lstm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from util.encoding import encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/features.csv\")\n",
    "X = df.iloc[:, 2:-1]  # skip index and name\n",
    "\n",
    "y = df[\"label\"]  # 10 genres\n",
    "y, code = encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NORMALIZE X ####\n",
    "# Normalize so everything is on the same scale.\n",
    "\n",
    "cols = X.columns\n",
    "std_scaler = sklearn.preprocessing.StandardScaler()\n",
    "np_scaled = std_scaler.fit_transform(X)\n",
    "\n",
    "# new data frame with the new scaled data. \n",
    "X = pd.DataFrame(np_scaled, columns = cols)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assess(model, X_train, X_test, y_train, y_test, title=\"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # binarize labels for multi class roc score\n",
    "    labels = list(set(y))\n",
    "    y_test_bin = label_binarize(y_test, classes=labels)\n",
    "    y_pred_bin = label_binarize(y_pred, classes=labels)\n",
    "    roc = roc_auc_score(y_test_bin, y_pred_bin, average=\"weighted\", multi_class=\"ovo\")\n",
    "    matt_cor = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{title}:\\n  Acc: {round(acc, 2)}\\n  F1: {round(f1, 2)}\\n  AUC score: {round(roc, 2)}\\n  Matth. corr. coeff.: {round(matt_cor, 2)}\"\n",
    "    )\n",
    "    return acc, f1, roc, matt_cor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save experiment results\n",
    "df_res = pd.DataFrame(columns=[\"classifier\", \"accuracy\", \"F1\", \"AUC\", \"MCC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log. Regression:\n",
      "  Acc: 0.59\n",
      "  F1: 0.6\n",
      "  ROC score: 0.77\n",
      "  Matth. corr. coeff.: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=0, max_iter=500).fit(X_train, y_train)\n",
    "log_reg_acc, log_reg_f1, log_reg_auc, log_reg_mcc = model_assess(\n",
    "    log_reg, X_train, X_test, y_train, y_test, title=\"Log. Regression\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Logistic regression\", log_reg_acc, log_reg_f1, log_reg_auc, log_reg_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian naive bayes:\n",
      "  Acc: 0.45\n",
      "  F1: 0.41\n",
      "  ROC score: 0.7\n",
      "  Matth. corr. coeff.: 0.4\n",
      "Log. Regression:\n",
      "  Acc: 0.45\n",
      "  F1: 0.41\n",
      "  ROC score: 0.7\n",
      "  Matth. corr. coeff.: 0.4\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_acc, gnb_f1, gnb_auc, gnb_mcc = model_assess(\n",
    "    gnb, X_train, X_test, y_train, y_test, title=\"Gaussian naive bayes\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Gaussian naive bayes\", gnb_acc, gnb_f1, gnb_auc, gnb_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian naive bayes:\n",
      "  Acc: 0.69\n",
      "  F1: 0.69\n",
      "  ROC score: 0.83\n",
      "  Matth. corr. coeff.: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = SVC(gamma=\"auto\")\n",
    "svc_acc, svc_f1, svc_auc, svc_mcc = model_assess(\n",
    "    svc_clf, X_train, X_test, y_train, y_test, title=\"SVC\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"SVC\", svc_acc, svc_f1, svc_auc, svc_mcc]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "  Acc: 0.69\n",
      "  F1: 0.69\n",
      "  ROC score: 0.83\n",
      "  Matth. corr. coeff.: 0.66\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier()\n",
    "forest_acc, forest_f1, forest_auc, forest_mcc = model_assess(\n",
    "    forest_clf, X_train, X_test, y_train, y_test, title=\"Random forest\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Random forest\", forest_acc, forest_f1, forest_auc, forest_mcc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Gradient Booster:\n",
      "  Acc: 0.7\n",
      "  F1: 0.7\n",
      "  ROC score: 0.83\n",
      "  Matth. corr. coeff.: 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7, 0.6987124151824312, 0.8340382415816421, 0.6698132500371287)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    booster=\"gbtree\",\n",
    "    learning_rate=0.04,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "model_assess(xgb, X_train, X_test, y_train, y_test, title=\"Cross Gradient Booster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Gradient Booster Random Forest:\n",
      "  Acc: 0.68\n",
      "  F1: 0.67\n",
      "  ROC score: 0.82\n",
      "  Matth. corr. coeff.: 0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6766666666666666,\n",
       " 0.6720489895367148,\n",
       " 0.8202430278423257,\n",
       " 0.6423984369768605)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbrf = XGBRFClassifier(\n",
    "    n_estimators=1000,\n",
    "    booster=\"gbtree\",\n",
    "    learning_rate=0.04,\n",
    "    objective=\"multi:softmax\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "model_assess(\n",
    "    xgbrf, X_train, X_test, y_train, y_test, \"Cross Gradient Booster Random Forest\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP:\n",
      "  Acc: 0.73\n",
      "  F1: 0.73\n",
      "  ROC score: 0.85\n",
      "  Matth. corr. coeff.: 0.7\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    activation=\"tanh\",\n",
    "    solver=\"adam\",\n",
    "    alpha=0.0001,\n",
    "    learning_rate=\"adaptive\",\n",
    "    learning_rate_init=0.01,\n",
    ")\n",
    "mlp_acc, mlp_f1, mlp_auc, mlp_mcc = model_assess(\n",
    "    mlp, X_train, X_test, y_train, y_test, title=\"MLP\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"MLP\", mlp_acc, mlp_f1, mlp_auc, mlp_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_539/2109828175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m lstm_acc, lstm_f1, lstm_auc, lstm_mcc = model_assess(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_539/3155335702.py\u001b[0m in \u001b[0;36mmodel_assess\u001b[0;34m(model, X_train, X_test, y_train, y_test, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_assess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/dat410/8_project/lstm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# caluclate the gradient, manually setting to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/dat410/8_project/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         h_0 = Variable(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         )  # hidden state\n\u001b[1;32m     71\u001b[0m         c_0 = Variable(\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not callable"
     ]
    }
   ],
   "source": [
    "X_train_t, X_test_t, y_train_t, y_test_t = prepare_data_lstm(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "lstm_params = LSTMNetParams(\n",
    "    num_epochs=2000,\n",
    "    learning_rate=0.01,\n",
    "    dropout=0.3,\n",
    "    input_size=29,\n",
    "    hidden_size=20,\n",
    "    hidden_layer=50,\n",
    "    num_layers=1,\n",
    "    num_classes=10,\n",
    "    seq_length=X_train_t.shape[1],\n",
    ")\n",
    "lstm = LSTMNet(lstm_params)\n",
    "lstm_acc, lstm_f1, lstm_auc, lstm_mcc = model_assess(\n",
    "    lstm, X_train, X_test, y_train, y_test, title=\"LSTM\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"LSTM\", lstm_acc, lstm_f1, lstm_auc, lstm_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Resnet\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv(\"data/classifier_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "744438a286f552de89f21840df11d95eed1d912f7f5940de34928fec5bf381d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
