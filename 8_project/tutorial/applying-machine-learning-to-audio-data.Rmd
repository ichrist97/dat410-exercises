---
title: "Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation"
description: |
  For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.
  ```{r, include = FALSE}
  bytes <- file.size("applying-machine-learning-to-audio-data.Rmd")
  words <- bytes/10
  minutes <- words/200
  ```
  (`r round(minutes)` min read)
author:
  - name: Tarid Wongvorachan
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    
date: 2021-12-11
categories:
  - Python
  - Data Visualization
  - Supervised Machine Learning
  
output:
  distill::distill_article:
    code_folding: true
    toc: true
    toc_depth: 2
    toc_float: false
    self_contained: false
---
## Machine Learning with Audio data  
  
  *  When we think of data, people may think of numbers and texts in tables. Some may even think of [using images as data](https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/), but just so you know, we can also convert and extract features from audio data (i.e., music) to understand and make use of it as well! Here, we will visualize music sound wave from .wav files to understand about what differentiates one tone from another (we can actually see soundwaves!).  
  
  *  I primarily relied on [Olteanu et al. (2019)'s work](https://www.kaggle.com/andradaolteanu/work-w-audio-data-visualise-classify-recommend), [Music genre classification article](https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8), and [Analytics Vidhya guide to the same topic](https://medium.com/analytics-vidhya/music-genre-classification-with-python-51bff77adfd6) to guide this reproduction and experimentation with the data.  
  
  *  To introduce the data set a bit. I will be using the GTZAN dataset, which is a public data set for evaluation in machine listening research for music genre recognition (MGR). The files were collected in 2000-2001 from a variety of sources including personal CDs, radio, microphone recordings to represent a variety of recording conditions.  
  
  *  We will start from importing audio data into our Python environment for data visualization; then, we will explore its feature such as sound wave, spectogram, mel-spectogram, harmonics and perceptrual, tempo, spectral centroid, and chroma frequencies. We will then conduct an exploratory data analysis with correlation heatmap with the extracted features, generating a box plot for genres distribution, and perform a principal component analysis to divide genres into groups.  
  
  *  Lastly, we will perform machine learning classification to train the algorithm to recognize and predict new audio files into genres (e.g., rock, pop, jazz), as well as develop a music recommendation system using the `cosine similarity` statistics. This function is a part of music delivery platforms such as Spotify, Youtube music, or Apple Music.  
  
```{r, include = FALSE}
setwd('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data')

```

  *  We will begin by importing necessary libraries for graphing (`seaborn` and `matplotlib`), data manipulation (`pandas`), machine learning (`sklearn`), and audio work (`librosa`).
  
```{python, warning = FALSE}
# Usual Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

import librosa
import librosa.display

```

## Explore Audio Data  
  
  *  We will use `librosa`, which is the main library for audio work in Python. Let us first Explore our Audio Data to see how it looks (we'll work with `pop.00002.wav` file). We will check for sound - the sequence of vibrations in varying pressure strengths (`y`) and sample rate (`sr`) the number of samples of audio carried per second, measured in Hz or kHz.  
  
```{python}
# Importing 1 file
y, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/pop/pop.00002.wav')

print('y:', y, '\n')
print('y shape:', np.shape(y), '\n')
print('Sample Rate (KHz):', sr, '\n')

# Verify length of the audio
print('Check Length of the audio in second:', 661794/22050)
```
  
  *  We will then clean the data by trimming all leading and trailing silence from the audio signal.  
  
```{python}
# Trim leading and trailing silence from an audio signal (silence before and after the actual audio)
audio_file, _ = librosa.effects.trim(y)

# the result is an numpy ndarray
print('Audio File:', audio_file, '\n')
print('Audio File shape:', np.shape(audio_file))
```
  
### 2D Representation: Sound Waves
  
  *  We can view a 2D representation of a sound with sound waves  
  
```{python}
plt.figure(figsize = (16, 6))
librosa.display.waveplot(y = audio_file, sr = sr, color = "#A300F9");
plt.title("Sound Waves in Pop 02", fontsize = 23);
plt.show()
```
  
### Fourier Transform  
  
  *  We will then perform a fourier transform to convert the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels.  
  
```{python}
# Default FFT window size
n_fft = 2048 # FFT window size
hop_length = 512 # number audio of frames between STFT columns (looks like a good default)

# Short-time Fourier transform (STFT)
D = np.abs(librosa.stft(audio_file, n_fft = n_fft, hop_length = hop_length))

print('Shape of D object:', np.shape(D))
```

```{python}
plt.figure(figsize = (16, 6))
plt.plot(D);
plt.show()
```
  
### The Spectrogram  
  
  *  Another characteristics that can represent a sound is its [spectogram](https://en.wikipedia.org/wiki/Spectrogram) - a visual representation of signal frequencies across time (aka sonographs, voiceprints, or voicegrams).  
  
```{python}
# Convert an amplitude spectrogram to Decibels-scaled spectrogram.
DB = librosa.amplitude_to_db(D, ref = np.max)

# Creating the Spectogram
plt.figure(figsize = (16, 6))
librosa.display.specshow(DB, sr = sr, hop_length = hop_length, x_axis = 'time', y_axis = 'log', cmap = 'cool')
plt.colorbar();
plt.show()
```

### Mel Spectrogram  
  
  *  The Mel Spectogram is a non-linear version of spectogram with a Mel scale on the y-axis. Mel scale converts the normal specrogram to frequencies that are perceptible by human ears, so basically, the difference between spectogram and mel spectogram is in its mathematical structure and its ability to be perceived by human. Each music genre has different spectogram (and mel spectogram) structure.  
  
```{python}
y, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/metal/metal.00036.wav')
y, _ = librosa.effects.trim(y)


S = librosa.feature.melspectrogram(y, sr=sr)
S_DB = librosa.amplitude_to_db(S, ref=np.max)
plt.figure(figsize = (16, 6))
librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',
                        cmap = 'cool');
plt.colorbar();
plt.title("Metal Mel Spectrogram", fontsize = 23);
plt.show()
```

```{python}
y, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/classical/classical.00036.wav')
y, _ = librosa.effects.trim(y)


S = librosa.feature.melspectrogram(y, sr=sr)
S_DB = librosa.amplitude_to_db(S, ref=np.max)
plt.figure(figsize = (16, 6))
librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',
                        cmap = 'cool');
plt.colorbar();
plt.title("Classical Mel Spectrogram", fontsize = 23);
plt.show()
```

## Audio Features  
  
  *  Now that we have explored an audio file with several visualizations of Spectogram, fourier transform, and sound waves, let us try extracting audio features that we may use with data manipulation and machine learning.  
  
### Zero Crossing Rate  
  
  *  the rate at which the sound signal changes from positive to negative and vice versa. This feature is usually used for speech recognition and music information retrieval. Music genre with high percussive sound like rock or metal usually have high Zero Crossing Rate than other genres.  
  
```{python}
# Total zero_crossings in our 1 song
zero_crossings = librosa.zero_crossings(audio_file, pad=False)
print(sum(zero_crossings))
```
  
### Harmonics and Perceptual  
  
  *  Harmonics (the orange wave) are audio characteristics that human ears can't distinguish (represents the sound color)  
  
  *  Perceptual (the purple wave) are sound waves that represent rhythm and emotion of the music.  
  
```{python, preview=TRUE}
y_harm, y_perc = librosa.effects.hpss(audio_file)

plt.figure(figsize = (16, 6))
plt.plot(y_harm, color = '#A300F9');
plt.plot(y_perc, color = '#FFB100');
plt.show()
```
  
### Tempo BMP (beats per minute)  
  
  *  Tempo is the number of beat per one minute.  
  
```{python}
tempo, _ = librosa.beat.beat_track(y, sr = sr)
tempo
```
  
### Spectral Centroid  
  
  *  This variable represents brightness of a sound by calculating the center of sound spectrum (where the sound signal is at its peak). We can also plot it into a wave form.  
  
```{python}
# Calculate the Spectral Centroids
spectral_centroids = librosa.feature.spectral_centroid(audio_file, sr=sr)[0]

# Shape is a vector
print('Centroids:', spectral_centroids, '\n')
print('Shape of Spectral Centroids:', spectral_centroids.shape, '\n')

# Computing the time variable for visualization
frames = range(len(spectral_centroids))

# Converts frame counts to time (seconds)
t = librosa.frames_to_time(frames)

print('frames:', frames, '\n')
print('t:', t)

# Function that normalizes the Sound Data
def normalize(x, axis=0):
    return sklearn.preprocessing.minmax_scale(x, axis=axis)
```

```{python}
#Plotting the Spectral Centroid along the waveform
plt.figure(figsize = (16, 6))
librosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');
plt.plot(t, normalize(spectral_centroids), color='#FFB100');
plt.show()
```
  
### Spectral Rolloff  
  
  *  Spectral Rolloff is a frequency below a specified percentage of the total spectral energy. It is like we have a cut-point, and we visualize the sound wave that is below that cut-point. Let's just call it as another characteristic of a sound.  
  
```{python}
# Spectral RollOff Vector
spectral_rolloff = librosa.feature.spectral_rolloff(audio_file, sr=sr)[0]

# The plot
plt.figure(figsize = (16, 6))
librosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');
plt.plot(t, normalize(spectral_rolloff), color='#FFB100');
plt.show()
```
  
### Mel-Frequency Cepstral Coefficients  
  
  *  The Mel frequency Cepstral coefficients (MFCCs) of a signal are a small set of features that describes the overall shape of a spectral envelope. It imitates characteristics of human voice.  
  
```{python}
mfccs = librosa.feature.mfcc(audio_file, sr=sr)
print('mfccs shape:', mfccs.shape)

#Displaying  the MFCCs:
plt.figure(figsize = (16, 6))
librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');
plt.show()
```
  
  *  We can scale the data a bit to make the feature (blue part) more apparent.  
  
```{python}
# Perform Feature Scaling
mfccs = sklearn.preprocessing.scale(mfccs, axis=1)
print('Mean:', mfccs.mean(), '\n')
print('Var:', mfccs.var())

plt.figure(figsize = (16, 6))
librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');
plt.show()
```

### Chroma Frequencies  
  
  *  Chroma feature represents the tone of music or sound by projecting its sound spectrum into a space that represents musical octave. This feature is usually used in chord recognition task.  
  
```{python}
# Increase or decrease hop_length to change how granular you want your data to be
hop_length = 5000

# Chromogram
chromagram = librosa.feature.chroma_stft(audio_file, sr=sr, hop_length=hop_length)
print('Chromogram shape:', chromagram.shape)

plt.figure(figsize=(16, 6))
librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm');
plt.show()
```

## Exploratory Data Analysis  
  
  *  We will perform an exploratory data analysis with the `features_30_sec.csv` data that contains the mean and variance of the features discussed above for all audio file in the data bank. We have 10 genres of music, each genre has 100 audio files. That makes the total of 1000 songs that we have. There are 60 features in total for each song.  
  
```{python}
data = pd.read_csv('features_30_sec.csv')
data.head()
```

  
### Correlation Heatmap for feature means  
  
  *  Here, we are making a correlation heatmap among feature means to see which feature correlates with which. The redder a square is, the more negative the correlation between that pair of variable becomes.  
  
```{python}

# Computing the Correlation Matrix
spike_cols = [col for col in data.columns if 'mean' in col]
corr = data[spike_cols].corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=np.bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(16, 11));

# Generate a custom diverging colormap
cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.title('Correlation Heatmap (for the MEAN variables)', fontsize = 25)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10);
plt.show()
```

### Box Plot for Genres Distributions  
  
  *  We will also make a boxplot for tempo of all music genres.  
  
```{python}
x = data[["label", "tempo"]]

f, ax = plt.subplots(figsize=(16, 9));
sns.boxplot(x = "label", y = "tempo", data = x, palette = 'husl');

plt.title('BPM Boxplot for Genres', fontsize = 25)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Genre", fontsize = 15)
plt.ylabel("BPM", fontsize = 15)
plt.show()
```

### Principal Component Analysis  
  
  *  For this part, we will conduct a principal component analysis (PCA) to visualize possible groups of genres and display its results with a scatter plot. We can see that a lot of songs have ambiguous genres; that is, it could be classified into more than one similar genres such as disco or hiphop based on the sound characteristics that we extract from them. There is also a song that is exclusively classified into a genre (reggae, for example).  
  
```{python}
from sklearn import preprocessing

data = data.iloc[0:, 1:]
y = data['label']
X = data.loc[:, data.columns != 'label']

#### NORMALIZE X ####
cols = X.columns
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
X = pd.DataFrame(np_scaled, columns = cols)


#### PCA 2 COMPONENTS ####
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

# concatenate with target label
finalDf = pd.concat([principalDf, y], axis = 1)

pca.explained_variance_ratio_

# 44.93 variance explained
```

```{python}
plt.figure(figsize = (16, 9))
sns.scatterplot(x = "principal component 1", y = "principal component 2", data = finalDf, hue = "label", alpha = 0.7,
               s = 100);

plt.title('PCA on Genres', fontsize = 25)
plt.xticks(fontsize = 14)
plt.yticks(fontsize = 10);
plt.xlabel("Principal Component 1", fontsize = 15)
plt.ylabel("Principal Component 2", fontsize = 15)
plt.show()
```

## Machine Learning Classification  
  
  *  Using features from `features_3_sec.csv` file, we can build a machine learning classification model that predicts genre of a new audio file. We will be loading a lot of machine learning models to see which model performs best.  
  
```{python}
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier, XGBRFClassifier
from xgboost import plot_tree, plot_importance

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
```

### Reading in the Data  
  
  *  We will read the data, split it into training and testing data sets, and create a function to assess accuracy of the models.  
  
```{python}
data = pd.read_csv('features_3_sec.csv')
data = data.iloc[0:, 1:] 
data.head()
```

### Features and Target variable  
  
  *  Create features and target variable, as well as normalizing the data.  
  
```{python}
y = data['label'] # genre variable.
X = data.loc[:, data.columns != 'label'] #select all columns but not the labels

#### NORMALIZE X ####

# Normalize so everything is on the same scale. 

cols = X.columns
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)

# new data frame with the new scaled data. 
X = pd.DataFrame(np_scaled, columns = cols)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

```{python}
#Creating a Predefined function to assess the accuracy of a model

def model_assess(model, title = "Default"):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    #print(confusion_matrix(y_test, preds))
    print('Accuracy', title, ':', round(accuracy_score(y_test, preds), 5), '\n')
```
  
  *  Here, we will test 10 different machine learning models to see which model is most suitable to music classification task.  
  
```{python}
# Naive Bayes
nb = GaussianNB()
model_assess(nb, "Naive Bayes")

# Stochastic Gradient Descent
sgd = SGDClassifier(max_iter=5000, random_state=0)
model_assess(sgd, "Stochastic Gradient Descent")

# KNN
knn = KNeighborsClassifier(n_neighbors=19)
model_assess(knn, "KNN")

# Decission trees
tree = DecisionTreeClassifier()
model_assess(tree, "Decission trees")

# Random Forest
rforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)
model_assess(rforest, "Random Forest")

# Support Vector Machine
svm = SVC(decision_function_shape="ovo")
model_assess(svm, "Support Vector Machine")

# Logistic Regression
lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')
model_assess(lg, "Logistic Regression")

# Neural Nets
nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)
model_assess(nn, "Neural Nets")

# Cross Gradient Booster
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, eval_metric='mlogloss')
model_assess(xgb, "Cross Gradient Booster")

# Cross Gradient Booster (Random Forest)
xgbrf = XGBRFClassifier(objective= 'multi:softmax', eval_metric='mlogloss')
model_assess(xgbrf, "Cross Gradient Booster (Random Forest)")
```
  
  *  The function Extreme Gradient Boosting (`XGBoost`) achieves the highest performance with 90% accuracy. We will be using this model to create the final prediction model and compute feature importance output along with its confusion matrix.  
  
  *  Note that I have also included Multilayer Perception - a variant of Neural Networks model - into the list of candidate models as well. While neural networks may be known for its complexity, it does not mean that the model is a silver bullet for every machine learning task. This idea is derived from the No Free Lunch Theorem that implies that there is no single best algorithm.  
  
```{python}
#Final model
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, eval_metric='mlogloss')
xgb.fit(X_train, y_train)


preds = xgb.predict(X_test)

print('Accuracy', ':', round(accuracy_score(y_test, preds), 5), '\n')

# Confusion Matrix
confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'
plt.figure(figsize = (16, 9))
sns.heatmap(confusion_matr, cmap="Blues", annot=True,
            xticklabels = ["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"],
           yticklabels=["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"]);
plt.show()
```

### Feature Importance  
  
  *  From the feature importance output, we can see that varianve and mean of the perceptual variable `perceptr_var` are the two most important variable in genre classification.  
  
```{python, warning = FALSE}
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(estimator=xgb, random_state=1)
perm.fit(X_test, y_test)

eli5.show_weights(estimator=perm, feature_names = X_test.columns.tolist())
```

## Music recommendation algorithm
  
  *  The music recommendation system assumes that the audience likes to listen to music of similar genres or similar characteristics. The system allows us to find the best similarity, ranked in descending order, from the bast match to the least best match with the `cosine_similarity` statistics.  
  
```{python}
# Libraries
import IPython.display as ipd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import preprocessing

# Read data
data = pd.read_csv('features_30_sec.csv', index_col='filename')

# Extract labels
labels = data[['label']]

# Drop labels from original dataframe
data = data.drop(columns=['length','label'])
data.head()

# Scale the data
data_scaled=preprocessing.scale(data)
print('Scaled data type:', type(data_scaled))
```

### Cosine Similarity  
  
  *  We will calculate the pairwise cosine similarity for each combination of songs in the data. The closer the value is to "1", the more similar the two songs can be.

```{python}
# Cosine similarity
similarity = cosine_similarity(data_scaled)
print("Similarity shape:", similarity.shape)

# Convert into a dataframe and then set the row index and column names as labels
sim_df_labels = pd.DataFrame(similarity)
sim_df_names = sim_df_labels.set_index(labels.index)
sim_df_names.columns = labels.index

sim_df_names.head()
```
  
### Song similarity scoring  
  
  *  We will define a function `find_similar_songs()` to take the name of the song and return top 5 best matches for that song.  
  
```{python}
def find_similar_songs(name):
    # Find songs most similar to another song
    series = sim_df_names[name].sort_values(ascending = False)
    
    # Remove cosine similarity == 1 (songs will always have the best match with themselves)
    series = series.drop(name)
    
    # Display the 5 top matches 
    print("\n*******\nSimilar songs to ", name)
    print(series.head(5))
```
  
  *  Now let us try putting it to the test:  
  
```{python}
find_similar_songs('pop.00023.wav') 

find_similar_songs('pop.00078.wav') 

find_similar_songs('rock.00018.wav') 

find_similar_songs('metal.00002.wav') 


```
  
  *  The output above shows similarity score for the sampled song. For example, the top three similar songs to `pop.00023` - Britney Spears - "**I'm so curious (2009 remaster)**" are `pop.00075`, `pop.00089`, and `pop.00088` respectively.  
  
  *  The algorithm can also recommend similar songs from other genres as well, for example, `metal.00002` - Iron Maiden "**Flight of Icarus**"has similar songs in both metal and rock genre. The same thing also applies to `rock.00018` - Queens - "**Another One Bites The Dust**" that has similar songs in both metal and rock genre as well.  
  
## Concluding note  
  
  *  It is interesting in how we are able to process audio data into numbers or images. The application of music recognition algorithm could be highly beneficial to entertainment industry in meeting the needs of consumer market. Researchers can also apply algorithm of this nature to extract characteristics that may be useful to their variable of interest such as attention or mental concentration.  
  
  *  One thing worth noting is, I am not a music expert, though I would love to practice piano at some point. The algorithm that I used is just one way of classifying musics into genres with the available information (e.g., tempo, harmonic wave). Domain expertise is important in data work regardless of your skill in data science. That is why it is crucial to consult with experts of the subject matter (i.e., musician) to make the most out of the insight we gained from this data. This also applies to other area such as testing as well. I can do the math and the programming, but I don't know much about students or English testing. This is where domain experts come into play. I just want to emphasize the importance of collaboration between fields to ensure the best results for the collective good.
  
  *  Due to the nature of my field (education), it is unlikely that I will have much chance to work with audio data, but this practice is still valuable regardless. The `model_assess` function that I used can be applied to any machine learning work that requires the use of several models to find the most suitable algorithm for the task. The `cosine_similarity` statistics is also useful to recommendation system of any products such as textbooks or novels. Anyway, it was a good practice, and I had fun nonetheless. As always, thank you very much for your read! I hope you have a good day wherever you are!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```




