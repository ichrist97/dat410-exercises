{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Union\n",
    "from functools import reduce\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warmup statistics for most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath: str) -> Tuple[List[str], List[str], List[List[str]]]:\n",
    "    words = []\n",
    "    sentences = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]  # strip whitespace and newline character\n",
    "\n",
    "        for line in lines:\n",
    "            remove = [\".\", \",\", \"!\", \"?\", \":\", \";\"]\n",
    "            new_words = list(filter(lambda x: x not in remove, line.split(\" \")))\n",
    "\n",
    "            # extend to one dimensional list to anaylze overall frequency of words\n",
    "            words.extend(new_words)\n",
    "\n",
    "            # append as separate sentence to keep sentence structure of words\n",
    "            sentences.append(new_words)\n",
    "\n",
    "        # filter unique words\n",
    "        unique = list(set(words))\n",
    "        unique.append(\"NULL\")\n",
    "    return words, unique, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# german text\n",
    "data_de_path = \"./data/europarl-v7.de-en.lc.de\"\n",
    "de_words, de_unique, de_sentences = read_file(data_de_path)\n",
    "\n",
    "# english text\n",
    "data_en_path = \"./data/europarl-v7.de-en.lc.en\"\n",
    "en_words, en_unique, en_sentences = read_file(data_en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 10 words in german text:\n",
      "Word: 'die'; Count: 10521\n",
      "Word: 'der'; Count: 9374\n",
      "Word: 'und'; Count: 7028\n",
      "Word: 'in'; Count: 4175\n",
      "Word: 'zu'; Count: 3168\n",
      "Word: 'den'; Count: 2976\n",
      "Word: 'wir'; Count: 2863\n",
      "Word: 'daÃŸ'; Count: 2738\n",
      "Word: 'ich'; Count: 2670\n",
      "Word: 'das'; Count: 2669\n",
      "Most common 10 words in english text:\n",
      "Word: 'the'; Count: 19847\n",
      "Word: 'of'; Count: 9597\n",
      "Word: 'to'; Count: 9059\n",
      "Word: 'and'; Count: 7303\n",
      "Word: 'in'; Count: 6237\n",
      "Word: 'is'; Count: 4478\n",
      "Word: 'that'; Count: 4441\n",
      "Word: 'a'; Count: 4435\n",
      "Word: 'we'; Count: 3372\n",
      "Word: 'this'; Count: 3362\n"
     ]
    }
   ],
   "source": [
    "de_cnt = Counter(de_words)\n",
    "en_cnt = Counter(en_words)\n",
    "\n",
    "n = 10\n",
    "de_most_cmn = de_cnt.most_common(n)\n",
    "en_most_cmn = en_cnt.most_common(n)\n",
    "\n",
    "print(f\"Most common {n} words in german text:\")\n",
    "for word, cnt in de_most_cmn:\n",
    "    print(f\"Word: '{word}'; Count: {cnt}\")\n",
    "\n",
    "print(f\"Most common {n} words in english text:\")\n",
    "for word, cnt in en_most_cmn:\n",
    "    print(f\"Word: '{word}'; Count: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for 'zebra': 0.0\n",
      "Probability for 'speaker': 4.150567495773968e-05\n"
     ]
    }
   ],
   "source": [
    "p_zebra = en_cnt[\"zebra\"] / len(en_words)\n",
    "print(f\"Probability for 'zebra': {p_zebra}\")\n",
    "\n",
    "p_speaker = en_cnt[\"speaker\"] / len(en_words)\n",
    "print(f\"Probability for 'speaker': {p_speaker}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_cnt(words: List[str], pair: Tuple[str, str]) -> int:\n",
    "    cnt = 0\n",
    "    # stop at second last word\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i + 1]\n",
    "        if w1 == pair[0] and w2 == pair[1]:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def calc_prob_word(w_cur: str, w_prev: str, words_cnt: Counter) -> float:\n",
    "    pair_count = get_pair_cnt(words_cnt, (w_prev, w_cur))\n",
    "    solo_count = en_cnt[w_prev]\n",
    "\n",
    "    # is a word does not occur at all, retrurn 0 probability\n",
    "    if solo_count == 0:\n",
    "        return 0\n",
    "\n",
    "    # maximum likelihood estimation\n",
    "    return pair_count / solo_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phrase_probs(sentence: List[str], words_cnt: Counter) -> float:\n",
    "    probs = []\n",
    "    for i in range(len(sentence)):\n",
    "        if i == 0:\n",
    "            probs.append(words_cnt[sentence[i]])\n",
    "        else:\n",
    "            w1, w2 = sentence[i], sentence[i - 1]\n",
    "            probs.append(calc_prob_word(w1, w2))\n",
    "\n",
    "    return reduce(lambda a, b: a * b, probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sentence = \"le chat noir\"\n",
    "fr_sentence = fr_sentence.split(\" \")\n",
    "eng_sentence = \"the black cat\"\n",
    "eng_sentence = eng_sentence.split(\" \")\n",
    "\n",
    "\n",
    "def init_c(a1: List[str], a2: List[str]) -> Dict[Union[Tuple[str, str], str], float]:\n",
    "    combos = list(product(a1, a2))\n",
    "    c = {(k1, k2): 0 for k1, k2 in combos}\n",
    "    e = {e: 0 for e in a1}\n",
    "    c.update(e)\n",
    "    return c\n",
    "\n",
    "\n",
    "def reestimate_t(t, c):\n",
    "    combo_keys = list(filter(lambda x: isinstance(x, tuple), c.keys()))\n",
    "    for key_pair in combo_keys:\n",
    "        orig_key, trans_key = key_pair[0], key_pair[1]\n",
    "        t.loc[orig_key][trans_key] = c[key_pair] / c[trans_key]\n",
    "    return t\n",
    "\n",
    "\n",
    "def em_iterations(\n",
    "    orig_uniques: List[str],\n",
    "    trans_uniques: List[str],\n",
    "    orig_corpus: List[List[str]],\n",
    "    trans_corpus: List[List[str]],\n",
    ") -> pd.DataFrame:\n",
    "    # random init T with dimension of unique words in training corpus\n",
    "    t_data = np.random.rand(len(orig_uniques), len(trans_uniques))\n",
    "    t = pd.DataFrame(t_data, columns=trans_uniques, index=orig_uniques)\n",
    "\n",
    "    # for each EM iteration (each unique word in a corpus)\n",
    "    for _ in range(50):\n",
    "    #for t_idx, t_row in t.iterrows():\n",
    "        # init pseudocounts\n",
    "        c = init_c(orig_uniques, trans_uniques)  # soft counts\n",
    "\n",
    "        # for each sentence\n",
    "        for k in range(len(orig_corpus)):\n",
    "            # for each orig word\n",
    "            for i in range(len(orig_uniques)):\n",
    "                # for each trans word\n",
    "                for j in range(len(trans_uniques)):\n",
    "                    # calc alignment prob and update pseudocount\n",
    "                    orig_word, trans_word = orig_corpus[k][i], trans_corpus[k][j]\n",
    "                    delta = t.loc[orig_word][trans_word] / t.loc[orig_word].sum()\n",
    "\n",
    "                    orig_word = orig_corpus[k][i]\n",
    "                    trans_word = trans_corpus[k][j]\n",
    "                    c[(orig_word, trans_word)] += delta\n",
    "                    c[orig_word] += delta\n",
    "\n",
    "        # reestimate t probs\n",
    "        t = reestimate_t(t, c)\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = em_iterations(de_unique, en_unique, de_sentences, en_sentences)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[1,2], [3,4]], columns=[\"trans1\", \"trans2\"], index=[\"orig1\", \"orig2\"])\n",
    "df.loc[\"orig1\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('c', 'd')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = {(\"a\", \"b\"): 0.54, (\"c\", \"d\"): 0.34, \"f\": 0.23}\n",
    "k = list(filter(lambda x: isinstance(x, tuple), c.keys()))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sentence = \"le chat noir\"\n",
    "fr_sentence = fr_sentence.split(\" \")\n",
    "eng_sentence = \"the black cat\"\n",
    "eng_sentence = eng_sentence.split(\" \")\n",
    "\n",
    "\n",
    "def init_c(a1: List[str], a2: List[str]) -> Dict[Union[Tuple[str, str], str], float]:\n",
    "    combos = list(product(a1, a2))\n",
    "    c = {(k1, k2): 0 for k1, k2 in combos}\n",
    "    e = {e: 0 for e in a1}\n",
    "    c.update(e)\n",
    "    return c\n",
    "\n",
    "def reestimate_t(t, c):\n",
    "    combo_keys = \n",
    "\n",
    "\n",
    "def em_iterations(\n",
    "    orig_prhase: List[str],\n",
    "    trans_phrase: List[str],\n",
    "    orig_uniques: List[str],\n",
    "    trans_uniques: List[str],\n",
    "    orig_corpus: List[List[str]],\n",
    "    trans_corpus: List[List[str]],\n",
    ") -> np.ndarray:\n",
    "    # random init T with dimension of unique words in training corpus\n",
    "    t_data = np.random.rand(len(orig_uniques), len(trans_uniques))\n",
    "    t = pd.DataFrame(t_data, columns=trans_uniques, index=orig_uniques)\n",
    "\n",
    "    # for each EM iteration (each unique word in a corpus)\n",
    "    for t_idx, t_row in t.iterrows():\n",
    "        # init pseudocounts\n",
    "        c = init_c(orig_uniques, trans_uniques)  # soft counts\n",
    "\n",
    "        # for each sentence\n",
    "        for k in range(len(orig_corpus)):\n",
    "            # for each orig word\n",
    "            for i in range(len(orig_uniques)):\n",
    "                # for each trans word\n",
    "                for j in range(len(trans_uniques)):\n",
    "                    # calc alignment prob and update pseudocount\n",
    "                    orig_word, trans_word = orig_corpus[k][i], trans_corpus[k][j]\n",
    "                    delta = t.loc[orig_word][trans_word] / t.loc[orig_word].sum()\n",
    "\n",
    "                    orig_word = orig_corpus[k][i]\n",
    "                    trans_word = trans_corpus[k][j]\n",
    "                    c[(orig_word, trans_word)] += delta\n",
    "                    c[orig_word] += delta\n",
    "\n",
    "        # reestimate t probs\n",
    "        t.loc[]\n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800288311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "np.sum(a[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "744438a286f552de89f21840df11d95eed1d912f7f5940de34928fec5bf381d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
